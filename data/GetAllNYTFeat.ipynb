{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1daaf0b6",
   "metadata": {},
   "source": [
    "***Description***\n",
    "\n",
    "<div> This notebook extracts three features from text in the articles: sentiment (VADER; Hutto & Gilbert 2014), (Stanford POS tagger; Toutanova et al. 2003), argumentation features (AlKhatib et al. 2016; Alhindi et al. 2020), on sentence level.\n",
    "\n",
    "<div> It consists of helper functions for importing data, importing argmentation feature detecting models (hence, ArgFeat models), extracting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9601aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/users/rldall/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/users/rldall/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/users/rldall/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /home/users/rldall/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "from glob import glob\n",
    "from nltk import word_tokenize, StanfordTagger\n",
    "from nltk.data import load\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn import preprocessing\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('tagsets')\n",
    "\n",
    "# import VADER\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# import POS tag_dict and label encoder\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(list(tagdict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "985ba576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show list of all POS tags and their descriptions\n",
    "print(nltk.help.upenn_tagset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f45ec7",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14892746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to import data\n",
    "\n",
    "def select_files(path, startwith):\n",
    "    list_of_files = []\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        if file.startswith(startwith):\n",
    "            list_of_files.append(str(path)+str(file))\n",
    "    return list_of_files\n",
    "\n",
    "def transform_input(file):\n",
    "    # import data\n",
    "    df = pd.read_csv(file, sep='\\t', header=None)\n",
    "    text_series = df[1]\n",
    "    text_token = []\n",
    "    # tokenize sentences\n",
    "    for t in text_series:\n",
    "        sent_token = sent_tokenize(t)\n",
    "        text_token.append(sent_token)\n",
    "    # new column\n",
    "    df[2] = text_token\n",
    "    # change labels\n",
    "    df[0] = df[0].map({'news':0 , 'editorial':1})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c91d43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to extract VADER sentiment\n",
    "\n",
    "def format_sent(compound_score):\n",
    "    polarity = 0\n",
    "    if(compound_score>= 0.05):\n",
    "        polarity = 1\n",
    "    elif(compound_score<= -0.05):\n",
    "        polarity = -1\n",
    "    return polarity\n",
    "\n",
    "def get_scores(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return np.array([scores.get(s) for s in scores])\n",
    "\n",
    "def get_sentiment(df,series_col,df_idx):\n",
    "    series = df[series_col]\n",
    "    error_list = []\n",
    "    compound_list = []\n",
    "    sum_list = []\n",
    "    \n",
    "#    for article in series:\n",
    "    for idx in range(len(series)):\n",
    "        article = series.iloc[idx]       \n",
    "        try:\n",
    "            scores = [get_scores(text) for text in article]\n",
    "            compound_list.append([s[-1] for s in scores])        \n",
    "            sum_list.append([format_sent(s[-1]) for s in scores])\n",
    "            \n",
    "        except:\n",
    "            print('Error line:',idx)\n",
    "            error_list.append(idx)\n",
    "\n",
    "    # new column\n",
    "    df['sent_compound'] = compound_list\n",
    "    df['sent_sum'] = sum_list\n",
    "    \n",
    "    df = df.drop(error_list)\n",
    "#    df.to_csv(list_of_files[df_idx],sep='\\t',header=False,index=False)\n",
    "    print('Saved\\t', list_of_files[df_idx].split('/')[-1])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad6a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to extract POS tags\n",
    "\n",
    "def predict_pos(text):\n",
    "    text_tok = nltk.word_tokenize(text)\n",
    "    return [word_class for word, word_class in nltk.pos_tag(text_tok)]\n",
    "\n",
    "def get_pos(df,series_col,df_idx):\n",
    "    series = df[series_col]\n",
    "    error_list = []\n",
    "    pos_list = []\n",
    "    for idx in range(len(series)):\n",
    "        article = series.iloc[idx]\n",
    "        try:\n",
    "            article_pos = [predict_pos(sent) for sent in article]\n",
    "            pos_list.append(article_pos)\n",
    "        except:\n",
    "            print('Error line:',idx)\n",
    "            error_list.append(idx)\n",
    "    # new column\n",
    "    df['pos'] = pos_list\n",
    "    df = df.drop(error_list)\n",
    "#    df.to_csv(list_of_files[df_idx],sep='\\t',header=False,index=False)\n",
    "    print('Saved\\t', list_of_files[df_idx].split('/')[-1])\n",
    "    return df\n",
    "\n",
    "# pad and buils np.array for each sentence\n",
    "def pad_sent(sent, max_padding):\n",
    "    sent_pos = predict_pos(sent)\n",
    "    sent_le = le.transform(sent_pos)\n",
    "    sent_le = [st for st in sent_le if st in list(le.classes_)] ####\n",
    "    sent_pos_padded = np.pad(np.array(sent_le), (0, max_padding- len(sent_pos)%max_padding) , 'constant', constant_values=(PAD_VALUE))\n",
    "    return sent_pos_padded\n",
    "\n",
    "def pad_article(article, max_padding, max_sent):\n",
    "    # input list from sent-token\n",
    "    art_pos_pad = np.empty(shape=(max_sent, max_padding))\n",
    "    art_pos_pad.fill(PAD_VALUE)\n",
    "    for i,sent in enumerate(article):\n",
    "        if i < max_sent:\n",
    "            art_pos_pad[i] = pad_sent(sent,max_padding)\n",
    "    return art_pos_pad\n",
    "\n",
    "# count method in each sent\n",
    "def counter_pos(article,max_sents):\n",
    "    a =[]\n",
    "    for idx,sent in enumerate(article):\n",
    "        sent_pos = predict_pos(sent)\n",
    "        count_pos = Counter(sent_pos)\n",
    "        a.append(dict(count_pos))\n",
    "    return a\n",
    "        \n",
    "def pos_count_article(counter_result, list_index,max_sents):\n",
    "    article_pos_count_array = np.zeros(shape=(len(le.classes_),max_sents))\n",
    "    for i,sent_pos_count in enumerate(counter_result):\n",
    "        for j in sent_pos_count:\n",
    "            article_pos_count_array[i,list_index.index(j)] = sent_pos_count.get(j)\n",
    "    return article_pos_count_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b58ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to extract argfeat prediction\n",
    "\n",
    "def load_model(name):\n",
    "    # name in form of numlabel_epochs\n",
    "    for f in os.listdir('/data/ArgFeatModel/ModelWeights/'):\n",
    "        if f.startswith('saved_weights_'+name):\n",
    "            model_path = ('/data/ArgFeatModel/ModelWeights/'+f)\n",
    "    loaded_model =  BertForSequenceClassification.from_pretrained('bert-base-cased',num_labels = int(name[0]))\n",
    "    loaded_model.load_state_dict(torch.load(model_path))\n",
    "    loaded_model.eval()\n",
    "    loaded_model.to(device)\n",
    "    return loaded_model\n",
    "\n",
    "# sent preprocessing\n",
    "def get_sent_argfeat(sent,tokenizer,model):\n",
    "    # token IDs and attention mask for inference on the new sentence\n",
    "    test_ids = []\n",
    "    test_attention_mask = []\n",
    "    # apply the tokenizer\n",
    "    encoding = tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 256,\n",
    "                        padding = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )\n",
    "    # extract IDs and attention mask\n",
    "    test_ids.append(encoding['input_ids'])\n",
    "    test_attention_mask.append(encoding['attention_mask'])\n",
    "    test_ids = torch.cat(test_ids, dim = 0)\n",
    "    test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "    with torch.no_grad():\n",
    "        output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
    "    # get prediction\n",
    "    pred = np.argmax(output.logits.cpu().numpy()).flatten().item()\n",
    "    return pred\n",
    "\n",
    "def get_argfeat(df,series_col,model, max_sent, df_idx):\n",
    "    print ('Extracting from', list_of_files[df_idx])\n",
    "    series = df[series_col]\n",
    "    error_list = []\n",
    "    count_long = 0\n",
    "    pred_list = []\n",
    "    error_list = []\n",
    "    for idx in range(len(series)):\n",
    "        article = series.iloc[idx]\n",
    "        if len(article) > max_sent:\n",
    "            try:\n",
    "                pred_text = [get_sent_argfeat(sent,tokenizer,model)+1 for sent in article[:max_sent]]\n",
    "                count_long += 1\n",
    "            except:\n",
    "                print('Error line:',idx)\n",
    "                error_list.append(idx)\n",
    "        else:\n",
    "            try:\n",
    "                pred_text = [get_sent_argfeat(sent,tokenizer,model)+1 for sent in article]# + [0] * (N-len(sent_token))\n",
    "            except:\n",
    "                print('Error line:',idx)\n",
    "                error_list.append(idx)\n",
    "        pred_list.append(pred_text)\n",
    "    print('long articles:',count_long,'from',len(df))\n",
    "    print('percent of long articles:', count_long/(count_long+len(df)))\n",
    "    flat_list = [item for sublist in pred_list for item in sublist]\n",
    "    num_label = max(flat_list)\n",
    "    # new column\n",
    "    df[str('argfeat'+str(num_label))] = pred_list\n",
    "    df = df.drop(error_list)\n",
    "#    df.to_csv(list_of_files[df_idx],sep='\\t',header=False,index=False)\n",
    "    print('Saved\\t', list_of_files[df_idx].split('/')[-1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26b16e",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fdcfe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf919d7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading sent...\n",
      "Saved\t train_finance.txt\n",
      "loading pos...\n",
      "Saved\t train_finance.txt\n",
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading argfeat...\n",
      "Extracting from /data/ProcessedNYT/train_finance.txt\n",
      "long articles: 38 from 2783\n",
      "percent of long articles: 0.01347040056717476\n",
      "Saved\t train_finance.txt\n",
      "Extracting from /data/ProcessedNYT/train_finance.txt\n",
      "long articles: 38 from 2783\n",
      "percent of long articles: 0.01347040056717476\n",
      "Saved\t train_finance.txt\n"
     ]
    }
   ],
   "source": [
    "MAXLEN= 100\n",
    "PAD_VALUE = 80\n",
    "MAX_SENT_PAD = 50\n",
    "MAX_SENTS = MAXLEN\n",
    "MAX_POS_PAD = 2000\n",
    "\n",
    "#select group of files\n",
    "list_of_files = select_files('/data/ProcessedNYT/','train_f') # processing training data (finance 1996;2005)\n",
    "#list_of_files = select_files('/data/ProcessedNYT/','all') # processing all data on six topics in 1986;1996;2005\n",
    "#list_of_files = select_files('/data/ProcessedNYT/','test') # processing six topics in 1986\n",
    "#print(list_of_files)\n",
    "\n",
    "# transform files into dataframes\n",
    "list_of_dataframes = [transform_input(file) for file in list_of_files]\n",
    "\n",
    "# get sentiment columns\n",
    "print('loading sent...')\n",
    "list_of_sent_dfs = [get_sentiment(df,2,df_idx) for df_idx,df in enumerate(list_of_dataframes)]\n",
    "\n",
    "# get pos columns\n",
    "print('loading pos...')\n",
    "list_of_pos_dfs = [get_pos(df,2,df_idx) for df_idx,df in enumerate(list_of_dataframes)]\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "# import arg-feat model\n",
    "print('loading model...')\n",
    "model3 = load_model(\"3_5\")\n",
    "model6 = load_model(\"6_3\")\n",
    "\n",
    "# get argfeat columns\n",
    "print('loading argfeat...')\n",
    "list_of_argfeat3_dfs = [get_argfeat(df,2, model3, MAX_SENTS, df_idx) for df_idx,df in enumerate(list_of_dataframes)]\n",
    "list_of_argfeat6_dfs = [get_argfeat(df,2, model6, MAX_SENTS, df_idx) for df_idx,df in enumerate(list_of_dataframes)]\n",
    "\n",
    "# final save\n",
    "for df_idx, df in enumerate(list_of_argfeat6_dfs):\n",
    "    df.to_csv(list_of_files[df_idx], sep='\\t',header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eddb4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
